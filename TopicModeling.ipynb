{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/domitillechambon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/domitillechambon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/domitillechambon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, nltk, lda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CombinedTop100Scrape\n",
    "raw = pd.read_csv(\"CombinedTop100Scrape.csv\")\n",
    "raw = raw.drop(\"Unnamed: 0\", axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def punctuation_extermination(s):\n",
    "    o = str.maketrans('', '', string.punctuation)\n",
    "    return str(s).translate(o)\n",
    "\n",
    "# Remove stopwords from columns within dataframes\n",
    "def removeStopWords(x):\n",
    "    t = x.split()\n",
    "    newTweet = []\n",
    "    for word in t:\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            newTweet.append(word)\n",
    "    newTweet = ' '.join(newTweet)\n",
    "    return newTweet\n",
    "\n",
    "# Cleaning Post Text column\n",
    "raw['Post Text'] = raw['Post Text'].apply(punctuation_extermination)\n",
    "raw['Post Text'] = raw['Post Text'].apply(lambda i: i.lower())\n",
    "raw['Post Text'] = raw['Post Text'].apply(lambda i: (removeStopWords(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Post Text column\n",
    "raw['Title'] = raw['Title'].apply(punctuation_extermination)\n",
    "raw['Title'] = raw['Title'].apply(lambda i: i.lower())\n",
    "raw['Title'] = raw['Title'].apply(lambda i: (removeStopWords(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Body column\n",
    "raw['Body'] = raw['Body'].apply(punctuation_extermination)\n",
    "raw['Body'] = raw['Body'].apply(lambda i: i.lower())\n",
    "raw['Body'] = raw['Body'].apply(lambda i: (removeStopWords(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting cleaned data\n",
    "raw.to_csv(\"CleanedTitleComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Cleaned Data\n",
    "cleanData = pd.read_csv(\"CleanedTitleComments.csv\")\n",
    "cleanData = cleanData.drop(\"Unnamed: 0\", axis= 1)\n",
    "cleanData[\"index\"] = range(1, len(cleanData) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dfTitle dataframe\n",
    "dfTitle = cleanData[[\"index\", \"Title\"]]\n",
    "dfTitle = dfTitle.rename(columns= {\"index\": \"id\", \"Title\": \"Labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with any of the empty columns:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#checking for nulls if present any\n",
    "print(\"Number of rows with any of the empty columns:\")\n",
    "print(dfTitle.isnull().sum().sum())\n",
    "dfTitle = dfTitle.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for two column names and number of topics\n",
    "restaurant_name = \"id\"\n",
    "restaurant_review = \"Labels\"\n",
    "ntopics= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizing functions\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that tokenizes the text\n",
    "def tokenize_text(version_desc):\n",
    "    lowercase=version_desc.lower()\n",
    "    text = wordnet_lemmatizer.lemmatize(lowercase)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55192, 404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Word to Vec variable created and total number of feeatures and words\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(dfTitle[restaurant_review])\n",
    "\n",
    "print(total_features_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 55192\n",
      "INFO:lda:vocab_size: 404\n",
      "INFO:lda:n_words: 355874\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -2796649\n",
      "INFO:lda:<10> log likelihood: -1761363\n",
      "INFO:lda:<20> log likelihood: -1661015\n",
      "INFO:lda:<30> log likelihood: -1644694\n",
      "INFO:lda:<40> log likelihood: -1640104\n",
      "INFO:lda:<50> log likelihood: -1639887\n",
      "INFO:lda:<60> log likelihood: -1640146\n",
      "INFO:lda:<70> log likelihood: -1639807\n",
      "INFO:lda:<80> log likelihood: -1639399\n",
      "INFO:lda:<90> log likelihood: -1638963\n",
      "INFO:lda:<100> log likelihood: -1639293\n",
      "INFO:lda:<110> log likelihood: -1639030\n",
      "INFO:lda:<120> log likelihood: -1638268\n",
      "INFO:lda:<130> log likelihood: -1638459\n",
      "INFO:lda:<140> log likelihood: -1638174\n",
      "INFO:lda:<150> log likelihood: -1638009\n",
      "INFO:lda:<160> log likelihood: -1638435\n",
      "INFO:lda:<170> log likelihood: -1637680\n",
      "INFO:lda:<180> log likelihood: -1637959\n",
      "INFO:lda:<190> log likelihood: -1637871\n",
      "INFO:lda:<200> log likelihood: -1637482\n",
      "INFO:lda:<210> log likelihood: -1638098\n",
      "INFO:lda:<220> log likelihood: -1637507\n",
      "INFO:lda:<230> log likelihood: -1638038\n",
      "INFO:lda:<240> log likelihood: -1637565\n",
      "INFO:lda:<250> log likelihood: -1637960\n",
      "INFO:lda:<260> log likelihood: -1637885\n",
      "INFO:lda:<270> log likelihood: -1637779\n",
      "INFO:lda:<280> log likelihood: -1638014\n",
      "INFO:lda:<290> log likelihood: -1638016\n",
      "INFO:lda:<300> log likelihood: -1638074\n",
      "INFO:lda:<310> log likelihood: -1638173\n",
      "INFO:lda:<320> log likelihood: -1638236\n",
      "INFO:lda:<330> log likelihood: -1638139\n",
      "INFO:lda:<340> log likelihood: -1638004\n",
      "INFO:lda:<350> log likelihood: -1637560\n",
      "INFO:lda:<360> log likelihood: -1637831\n",
      "INFO:lda:<370> log likelihood: -1638247\n",
      "INFO:lda:<380> log likelihood: -1637928\n",
      "INFO:lda:<390> log likelihood: -1637745\n",
      "INFO:lda:<400> log likelihood: -1638092\n",
      "INFO:lda:<410> log likelihood: -1637869\n",
      "INFO:lda:<420> log likelihood: -1637740\n",
      "INFO:lda:<430> log likelihood: -1638061\n",
      "INFO:lda:<440> log likelihood: -1637619\n",
      "INFO:lda:<450> log likelihood: -1637964\n",
      "INFO:lda:<460> log likelihood: -1638096\n",
      "INFO:lda:<470> log likelihood: -1637812\n",
      "INFO:lda:<480> log likelihood: -1637989\n",
      "INFO:lda:<490> log likelihood: -1638270\n",
      "INFO:lda:<499> log likelihood: -1637611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x165af0d30>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actualizing the model\n",
    "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)\n",
    "model.fit(total_features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "dfTitle=dfTitle.join(doc_topic)\n",
    "restaurant=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    restaurant[topic]=dfTitle.groupby([restaurant_name])[i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics word distribution written in file Title - topic_word_dist.csv \n",
      "Document topic distribution written in file Title - document_topic_dist.csv \n"
     ]
    }
   ],
   "source": [
    "restaurant=restaurant.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "print (\"Topics word distribution written in file Title - topic_word_dist.csv \")\n",
    "topics1.to_csv(\"Title - topic_word_dist.csv\")\n",
    "restaurant.to_csv(\"Title - document_topic_dist.csv\",index=False)\n",
    "print (\"Document topic distribution written in file Title - document_topic_dist.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling Post Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dfPostText dataframe\n",
    "dfPostText = cleanData[[\"index\", \"Post Text\"]]\n",
    "dfPostText = dfPostText.rename(columns= {\"index\": \"id\", \"Post Text\": \"Labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with any of the empty columns:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#checking for nulls if present any\n",
    "print(\"Number of rows with any of the empty columns:\")\n",
    "print(dfPostText.isnull().sum().sum())\n",
    "dfPostText = dfPostText.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for two column names and number of topics\n",
    "restaurant_name = \"id\"\n",
    "restaurant_review = \"Labels\"\n",
    "ntopics= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizing functions\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55192, 2768)\n"
     ]
    }
   ],
   "source": [
    "# Word to Vec variable created and total number of feeatures and words\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(dfPostText[restaurant_review])\n",
    "\n",
    "print(total_features_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 55192\n",
      "INFO:lda:vocab_size: 2768\n",
      "INFO:lda:n_words: 5017184\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -43660608\n",
      "INFO:lda:<10> log likelihood: -33729303\n",
      "INFO:lda:<20> log likelihood: -33044334\n",
      "INFO:lda:<30> log likelihood: -32897376\n",
      "INFO:lda:<40> log likelihood: -32784971\n",
      "INFO:lda:<50> log likelihood: -32767650\n",
      "INFO:lda:<60> log likelihood: -32755389\n",
      "INFO:lda:<70> log likelihood: -32743929\n",
      "INFO:lda:<80> log likelihood: -32726911\n",
      "INFO:lda:<90> log likelihood: -32703222\n",
      "INFO:lda:<100> log likelihood: -32688843\n",
      "INFO:lda:<110> log likelihood: -32678654\n",
      "INFO:lda:<120> log likelihood: -32664656\n",
      "INFO:lda:<130> log likelihood: -32657884\n",
      "INFO:lda:<140> log likelihood: -32653949\n",
      "INFO:lda:<150> log likelihood: -32642477\n",
      "INFO:lda:<160> log likelihood: -32639964\n",
      "INFO:lda:<170> log likelihood: -32629277\n",
      "INFO:lda:<180> log likelihood: -32630111\n",
      "INFO:lda:<190> log likelihood: -32630955\n",
      "INFO:lda:<200> log likelihood: -32619981\n",
      "INFO:lda:<210> log likelihood: -32611926\n",
      "INFO:lda:<220> log likelihood: -32598638\n",
      "INFO:lda:<230> log likelihood: -32589386\n",
      "INFO:lda:<240> log likelihood: -32590129\n",
      "INFO:lda:<250> log likelihood: -32590009\n",
      "INFO:lda:<260> log likelihood: -32591473\n",
      "INFO:lda:<270> log likelihood: -32590464\n",
      "INFO:lda:<280> log likelihood: -32588872\n",
      "INFO:lda:<290> log likelihood: -32591352\n",
      "INFO:lda:<300> log likelihood: -32589668\n",
      "INFO:lda:<310> log likelihood: -32587157\n",
      "INFO:lda:<320> log likelihood: -32589672\n",
      "INFO:lda:<330> log likelihood: -32584397\n",
      "INFO:lda:<340> log likelihood: -32577166\n",
      "INFO:lda:<350> log likelihood: -32576792\n",
      "INFO:lda:<360> log likelihood: -32576198\n",
      "INFO:lda:<370> log likelihood: -32574543\n",
      "INFO:lda:<380> log likelihood: -32577178\n",
      "INFO:lda:<390> log likelihood: -32577179\n",
      "INFO:lda:<400> log likelihood: -32574487\n",
      "INFO:lda:<410> log likelihood: -32572528\n",
      "INFO:lda:<420> log likelihood: -32574871\n",
      "INFO:lda:<430> log likelihood: -32577663\n",
      "INFO:lda:<440> log likelihood: -32576500\n",
      "INFO:lda:<450> log likelihood: -32574494\n",
      "INFO:lda:<460> log likelihood: -32572092\n",
      "INFO:lda:<470> log likelihood: -32577019\n",
      "INFO:lda:<480> log likelihood: -32577788\n",
      "INFO:lda:<490> log likelihood: -32575161\n",
      "INFO:lda:<499> log likelihood: -32575251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x2941d5a80>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actualizing the model\n",
    "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)\n",
    "model.fit(total_features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "dfPostText=dfPostText.join(doc_topic)\n",
    "restaurant=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    restaurant[topic]=dfPostText.groupby([restaurant_name])[i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics word distribution written in file Post Text - topic_word_dist.csv \n",
      "Document topic distribution written in file Post Text - document_topic_dist.csv \n"
     ]
    }
   ],
   "source": [
    "restaurant=restaurant.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "print (\"Topics word distribution written in file Post Text - topic_word_dist.csv \")\n",
    "topics1.to_csv(\"Post Text - topic_word_dist.csv\")\n",
    "restaurant.to_csv(\"Post Text - document_topic_dist.csv\",index=False)\n",
    "print (\"Document topic distribution written in file Post Text - document_topic_dist.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dfBody dataframe\n",
    "dfBody = cleanData[[\"index\", \"Body\"]]\n",
    "dfBody = dfBody.rename(columns= {\"index\": \"id\", \"Body\": \"Labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with any of the empty columns:\n",
      "337\n"
     ]
    }
   ],
   "source": [
    "#checking for nulls if present any\n",
    "print(\"Number of rows with any of the empty columns:\")\n",
    "print(dfBody.isnull().sum().sum())\n",
    "dfBody = dfBody.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for two column names and number of topics\n",
    "restaurant_name = \"id\"\n",
    "restaurant_review = \"Labels\"\n",
    "ntopics= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizing functions\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54855, 33076)\n"
     ]
    }
   ],
   "source": [
    "# Word to Vec variable created and total number of feeatures and words\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(dfBody[restaurant_review])\n",
    "\n",
    "print(total_features_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 54855\n",
      "INFO:lda:vocab_size: 33076\n",
      "INFO:lda:n_words: 998891\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -9931262\n",
      "INFO:lda:<10> log likelihood: -8687354\n",
      "INFO:lda:<20> log likelihood: -8566889\n",
      "INFO:lda:<30> log likelihood: -8496695\n",
      "INFO:lda:<40> log likelihood: -8437679\n",
      "INFO:lda:<50> log likelihood: -8392631\n",
      "INFO:lda:<60> log likelihood: -8358450\n",
      "INFO:lda:<70> log likelihood: -8333913\n",
      "INFO:lda:<80> log likelihood: -8315751\n",
      "INFO:lda:<90> log likelihood: -8301897\n",
      "INFO:lda:<100> log likelihood: -8292562\n",
      "INFO:lda:<110> log likelihood: -8285331\n",
      "INFO:lda:<120> log likelihood: -8277307\n",
      "INFO:lda:<130> log likelihood: -8270391\n",
      "INFO:lda:<140> log likelihood: -8267794\n",
      "INFO:lda:<150> log likelihood: -8266851\n",
      "INFO:lda:<160> log likelihood: -8263048\n",
      "INFO:lda:<170> log likelihood: -8259582\n",
      "INFO:lda:<180> log likelihood: -8256876\n",
      "INFO:lda:<190> log likelihood: -8254373\n",
      "INFO:lda:<200> log likelihood: -8254750\n",
      "INFO:lda:<210> log likelihood: -8253102\n",
      "INFO:lda:<220> log likelihood: -8250374\n",
      "INFO:lda:<230> log likelihood: -8250690\n",
      "INFO:lda:<240> log likelihood: -8248931\n",
      "INFO:lda:<250> log likelihood: -8247429\n",
      "INFO:lda:<260> log likelihood: -8247789\n",
      "INFO:lda:<270> log likelihood: -8245084\n",
      "INFO:lda:<280> log likelihood: -8245187\n",
      "INFO:lda:<290> log likelihood: -8245189\n",
      "INFO:lda:<300> log likelihood: -8242494\n",
      "INFO:lda:<310> log likelihood: -8242417\n",
      "INFO:lda:<320> log likelihood: -8242091\n",
      "INFO:lda:<330> log likelihood: -8241157\n",
      "INFO:lda:<340> log likelihood: -8241454\n",
      "INFO:lda:<350> log likelihood: -8241412\n",
      "INFO:lda:<360> log likelihood: -8239368\n",
      "INFO:lda:<370> log likelihood: -8238430\n",
      "INFO:lda:<380> log likelihood: -8238217\n",
      "INFO:lda:<390> log likelihood: -8237480\n",
      "INFO:lda:<400> log likelihood: -8238025\n",
      "INFO:lda:<410> log likelihood: -8236810\n",
      "INFO:lda:<420> log likelihood: -8238186\n",
      "INFO:lda:<430> log likelihood: -8235338\n",
      "INFO:lda:<440> log likelihood: -8234716\n",
      "INFO:lda:<450> log likelihood: -8234283\n",
      "INFO:lda:<460> log likelihood: -8234709\n",
      "INFO:lda:<470> log likelihood: -8235045\n",
      "INFO:lda:<480> log likelihood: -8236041\n",
      "INFO:lda:<490> log likelihood: -8234987\n",
      "INFO:lda:<499> log likelihood: -8235667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x2941d6f50>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actualizing the model\n",
    "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1)\n",
    "model.fit(total_features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "dfBody=dfBody.join(doc_topic)\n",
    "restaurant=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    restaurant[topic]=dfBody.groupby([restaurant_name])[i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics word distribution written in file Body - topic_word_dist.csv \n",
      "Document topic distribution written in file Body - document_topic_dist.csv \n"
     ]
    }
   ],
   "source": [
    "restaurant=restaurant.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "print (\"Topics word distribution written in file Body - topic_word_dist.csv \")\n",
    "topics1.to_csv(\"Body - topic_word_dist.csv\")\n",
    "restaurant.to_csv(\"Body - document_topic_dist.csv\",index=False)\n",
    "print (\"Document topic distribution written in file Body - document_topic_dist.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quartile & Topic Weights based on Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First quartile: 0.25    2230.0\n",
      "Name: Title Score, dtype: float64\n",
      "Third quartile: 0.75    3664.0\n",
      "Name: Title Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Finding first and third quartiles\n",
    "print(\"First quartile:\", cleanData[\"Title Score\"].quantile([.25]))\n",
    "print(\"Third quartile:\", cleanData[\"Title Score\"].quantile([.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting first and third quartile scores\n",
    "firstQuartile = float(2230)\n",
    "thirdQuartile = float(3664)\n",
    "\n",
    "# Dataframe with values from first and third quartiles\n",
    "dfQuartile = cleanData[(cleanData[\"Title Score\"] <= firstQuartile) | (cleanData[\"Title Score\"] >= thirdQuartile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying quartile\n",
    "def quart(val):\n",
    "    \"\"\" Determine which quartile the image belongs to\"\"\"\n",
    "    if val >= thirdQuartile:\n",
    "        return \"third\"\n",
    "    elif val <= firstQuartile:\n",
    "        return \"first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>ID</th>\n",
       "      <th>Title Score</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Post URL</th>\n",
       "      <th>User</th>\n",
       "      <th>Body</th>\n",
       "      <th>Comment Score</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>someone really likes it’s effortless</td>\n",
       "      <td>keep mind text right away soon excited make pl...</td>\n",
       "      <td>sgcach</td>\n",
       "      <td>11302</td>\n",
       "      <td>508</td>\n",
       "      <td>https://www.reddit.com/r/dating_advice/comment...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>welcome rdatingadvice please keep ruleshttpsww...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title  \\\n",
       "0  someone really likes it’s effortless   \n",
       "\n",
       "                                           Post Text      ID  Title Score  \\\n",
       "0  keep mind text right away soon excited make pl...  sgcach        11302   \n",
       "\n",
       "   Total Comments                                           Post URL  \\\n",
       "0             508  https://www.reddit.com/r/dating_advice/comment...   \n",
       "\n",
       "            User                                               Body  \\\n",
       "0  AutoModerator  welcome rdatingadvice please keep ruleshttpsww...   \n",
       "\n",
       "   Comment Score  index  \n",
       "0              1      1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfQuartile.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/8fq8vkds1wschn5rrzxkc8br0000gn/T/ipykernel_80168/1765748416.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfQuartile[\"Quartile\"] = dfQuartile[\"Title Score\"].apply(lambda x: quart(x))\n"
     ]
    }
   ],
   "source": [
    "# Labeling first or third quartile\n",
    "dfQuartile[\"Quartile\"] = dfQuartile[\"Title Score\"].apply(lambda x: quart(x))\n",
    "\n",
    "# Excluding unnecessary columns from dataframe\n",
    "dfQuartile = dfQuartile.drop([\"Post URL\", \"index\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in topic modeling csv\n",
    "topicModels = pd.read_csv(\"Title - document_topic_dist.csv\")\n",
    "\n",
    "# Concatenating two dataframes\n",
    "dfComb = pd.concat([dfQuartile, topicModels], axis= 1)\n",
    "\n",
    "# Removing unnecessary columns\n",
    "dfComb = dfComb[[\"Quartile\", \"topic_0\", \"topic_1\", \"topic_2\", \"topic_3\", \"topic_4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging topic values and grouping by first and third quartile\n",
    "dfAverage = dfComb.groupby(\"Quartile\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quartile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.147653</td>\n",
       "      <td>0.328940</td>\n",
       "      <td>0.238324</td>\n",
       "      <td>0.053237</td>\n",
       "      <td>0.231846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third</th>\n",
       "      <td>0.304480</td>\n",
       "      <td>0.126188</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>0.247426</td>\n",
       "      <td>0.141964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           topic_0   topic_1   topic_2   topic_3   topic_4\n",
       "Quartile                                                  \n",
       "first     0.147653  0.328940  0.238324  0.053237  0.231846\n",
       "third     0.304480  0.126188  0.179941  0.247426  0.141964"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average topic values per 2 quartiles pre topic naming\n",
    "dfAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <strong> Topics: </strong> </h4>\n",
    "<p> <strong> topic_0 - </strong> Red flags on first date </p>\n",
    "<p> <strong> topic_1 - </strong> Women having problem with men being too interested in sex </p>\n",
    "<p> <strong> topic_2 - </strong> Sexual insecurities </p>\n",
    "<p> <strong> topic_3 - </strong> Men focuesed on attractiveness </p>\n",
    "<p> <strong> topic_4 - </strong> Difficulties surrounding dating </p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<h4> <strong> Topic Value Assessment: </strong> </h4>\n",
    "<p> There are three significant differences between the two engagement quartiles in regard to the following topics: topic_0, topic_1, topic_3. Topic_0 and topic_3 have larger proportions in the third quartile (the higher engagement quartile). Topic_2 has a larger proportion in the first quartile (the lower engagement quartile). Topic_2 and topic_4 have larger proportions in the first quartile (the lower engagement quartile). </p>\n",
    "<p> <strong> Most popular topics: </strong> topic_0 & topic_3 </p>\n",
    "\n",
    "</br>\n",
    "\n",
    "<ol>\n",
    "<h5>High Engagement Keywords:</h5>\n",
    "\n",
    "<li>Date, First, Said, Red, Sex, Got, Time, Already, Body, Fupa, Negative, Things, Weeks, Flag, Problems, Guy, Friends, Benefits, Creating</li>\n",
    "<li>Guy, Men, Want, Attractive, Every, Please, Think, Age, Annoying, F, Fucking, Fun, God, Share, Threesome, Every, Women, Went, Another</li>\n",
    "\n",
    "\n",
    "<h5>Low Engagement Keywords:</h5>\n",
    "\n",
    "<li>Woman, Sex, Men, Reddit, Turn, Dates, Dating, Stop, Talk, I'm, Wants, Please, People, Talking, Interested, Still, Soon, Nice, Tell, 25f</li>\n",
    "<li>First, Date, Girl, Would, Lesser, Sex, Think, Feel, Situation, Showed, Make, Move, Tinder, Life, Makeup, Hasn't, Kiss, Know, Let, Micropenis</li>\n",
    "<li>Women, Dating, Guy, Difficult, Thing, What's, Man, Extremely, Way, Girls, Looks, Beside, Makes, Attractive, You're, Friend, Initiate, Best, Someone, Told</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
